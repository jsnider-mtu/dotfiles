#!/bin/bash

if [[ ! -d ~/.a ]]; then
  mkdir ~/.a
fi

highestres() {

  curl -s -c ~/.tmp/cookies "$1"|grep RESOLUTION|sed 's/^.*,NAME="\(.*\)"$/\1/'|sort -h|tail -1

}

leadingzeroes() {

  for ((i=0;i<=LAST_NUM;i++)); do
    if [[ $i -lt 10 ]]; then
      mv ~/.tmp/hls-$HIGHEST_RES$i.ts ~/.tmp/00$i.ts
    elif [[ $i -lt 100 ]]; then
      mv ~/.tmp/hls-$HIGHEST_RES$i.ts ~/.tmp/0$i.ts
    else
      mv ~/.tmp/hls-$HIGHEST_RES$i.ts ~/.tmp/$i.ts
    fi
  done

}

if [ $# -ne 1 ]; then
    echo "Fail! -- Expecting 1 argument! ==> $@"
    exit 1
fi

if [ -z "`which curl`" ]; then
    echo "Fail! -- Requires \"curl\""
    echo "Hint: sudo aptitude install curl"
    exit 1
fi

URL=$1

echo ""
echo "Extracting video URL from.. ==> $URL"

# Should probably check for space on file system before the download is attempted

# Changing this declaration to account for site changes
# For one, returned html is pre-formatted, so just do the
# curl  and grep and be done with it
HLS_URL=`curl -s -c ~/.tmp/cookies "$URL"     | # download HTML using curl
        grep 'html5player.setVideoHLS'        | # locate lines with "html5player.setVideoHLS"
        cut -d\' -f2`                           # clean up a little

# Turns out video titles (therefore: URLs) can change and nothing is returned for HLS_URL
# We should probably exit here and explain the situation
if [[ -z $HLS_URL ]]; then
  echo -e "\nNo HLS URL found, title probably changed\n"
  exit 3
fi

# Need to use HLS_URL and xvres-show to get the highest resolution
# Then grab that manifest file to get a list of segments (or just the last in the enumeration
# since they've already standardized the naming scheme for me)
HIGHEST_RES=`highestres "$HLS_URL"`  # This needs to actually recreate the URL entirely
#echo "DEBUG: HIGHEST_RES = $HIGHEST_RES"
if [[ -f ~/.a/${URL##*/}_${HIGHEST_RES}.mp4 ]]; then
  echo -e "\nFile already exists, fuck extra work\n"
  exit 2
fi
if [[ $HIGHEST_RES =~ ^[23]{1} ]]; then  # If highest resolution begins with 2 or 3 then skip this video
  echo -e "\nNot high enough quality, fuck this video\n"
  exit 1
fi

# Need to form the next url and then parse the response for last segment
# second to last line
#echo "DEBUG: HLS_URL = $HLS_URL"
HIGHEST_URL=`echo "${HLS_URL/hls.m3u8/hls-$HIGHEST_RES.m3u8}"` # URL Encode fixes
#echo "DEBUG: HIGHEST_URL = $HIGHEST_URL"

LAST_SEG=`curl -s -c ~/.tmp/cookies "$HIGHEST_URL"            | # download HTML using curl
          tail -2|head -1`                                      # just need the last segment
#echo "DEBUG: LAST_SEG = $LAST_SEG"

# Need to grab any and all parameters at the end of the URL
if [[ $LAST_SEG =~ .*\?.* ]]; then
  PARAMETERS=`echo $LAST_SEG|cut -d'?' -f2-`
#  echo "DEBUG: PARAMETERS = $PARAMETERS"
  REAL_LAST_SEG=`echo $LAST_SEG|cut -d'?' -f1`
#  echo "DEBUG: REAL_LAST_SEG = $REAL_LAST_SEG"
fi

mkdir ~/.tmp 2>/dev/null

# Now let's generate the enumerated filename list and download in parallel
if [[ -z $PARAMETERS ]]; then
  LAST_NUM=`echo $LAST_SEG|rev|sed 's/^st\.\([0-9]\{1,\}\).*$/\1/'|rev`
#  echo "DEBUG: LAST_NUM = $LAST_NUM"
  for ((i=0;i<=LAST_NUM;i++)); do echo ${HLS_URL/hls.m3u8/hls-$HIGHEST_RES$i}.ts; done | xargs -n 1 -P 8 wget -q -P ~/.tmp
  leadingzeroes $LAST_NUM
else
  LAST_NUM=`echo $REAL_LAST_SEG|rev|sed 's/^st\.\([0-9]\{1,\}\).*$/\1/'|rev`
#  echo "DEBUG: LAST_NUM = $LAST_NUM"
  for ((i=0;i<=LAST_NUM;i++)); do echo ${HLS_URL/hls.m3u8/hls-$HIGHEST_RES${i}.ts}; done | xargs -n 1 -P 8 wget -q -P ~/.tmp
  for ((i=0;i<=LAST_NUM;i++)); do mv ~/.tmp/hls-$HIGHEST_RES${i}.ts* ~/.tmp/hls-$HIGHEST_RES${i}.ts; done
  leadingzeroes $LAST_NUM
fi

cat $(ls -vl ~/.tmp/*ts|awk '{print $9}'|xargs) > ~/.a/${URL##*/}.ts
rm -r ~/.tmp

# Now re-encode to get rid of the choppy bits
# This part sucks, replacing with ffmpeg
ffmpeg -loglevel 24 -i ~/.a/${URL##*/}.ts -acodec copy -vcodec copy ~/.a/${URL##*/}_${HIGHEST_RES}.mp4

echo ""
echo "Done, you motherfucker. Cleaning up now."
rm ~/.a/${URL##*/}.ts
